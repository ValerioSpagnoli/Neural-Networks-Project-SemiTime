{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.io.arff import loadarff\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, ConcatDataset, SubsetRandomSampler, random_split\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "from torchmetrics.classification import BinaryAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRICKET_X = 'CricketX'\n",
    "INSECT_WINGBEAT_SOUND = 'InsectWingbeatSound'\n",
    "U_WAVE_GESTURE_LIBRARY_ALL = 'UWaveGestureLibraryAll'\n",
    "#EPILEPRIC_SEIZURE= 'EpilepticSeizure'\n",
    "#MFPT = 'MFPT'\n",
    "#XJTU = 'XJTU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_features = 64\n",
    "num_epochs = 1000\n",
    "patience = 200 # num of epochs with prev_loss - loss <= threshold without stopping\n",
    "threshold = 0.0001\n",
    "learning_rate = 0.01\n",
    "alpha = 0.4 #past-future segment split ratio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgb(0, 255, 255, 0.2);\"><h5>def load_labeledDataframe()</h5></div>\n",
    "\n",
    "Provides the labeled version of dataset.\n",
    "\n",
    "Takes: \n",
    "- dataset: name of dataset (e.g. CRICKET_X)\n",
    "\n",
    "Returns:\n",
    "- df_train: pandas dataframe of train samples\n",
    "- df_test: pandas dataframe of test samples\n",
    "- num_classes: number of classes of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labeledDataframe(dataset = None):\n",
    "\n",
    "    print(f'Dataset: {dataset}')\n",
    "\n",
    "    #load dataset\n",
    "    train = np.loadtxt(f'../Datasets/{dataset}/{dataset}_TRAIN.tsv').astype(np.float32)\n",
    "    test = np.loadtxt(f'../Datasets/{dataset}/{dataset}_TEST.tsv').astype(np.float32)\n",
    "\n",
    "    #divide dataset in attributes and labels\n",
    "    X_train = train[:, 1:] # each row, columns from 1 to n\n",
    "    y_train = np.squeeze(train[:, 0:1],axis=1).astype(np.int32) # each row, columns 0\n",
    "    X_test = test[:, 1:] # each row, columns from 1 to n\n",
    "    y_test= np.squeeze(test[:, 0:1],axis=1).astype(np.int32) # each row, columns 0\n",
    "\n",
    "    #encodes labels between 0 and n-1\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    encoder.fit(y_train)\n",
    "    y_train = encoder.transform(y_train)\n",
    "    encoder.fit(y_test)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    #create a list of name for attributes\n",
    "    attributes = [f'att{i}' for i in range(len(X_train[0]))]\n",
    "\n",
    "    #create a pandas dataframe for train set\n",
    "    df_Xtrain = pd.DataFrame(X_train, columns=attributes)\n",
    "    df_ytrain = pd.DataFrame(y_train, columns=['target'])\n",
    "    df_train = pd.concat([df_Xtrain, df_ytrain], axis = 1)\n",
    "\n",
    "    #create a pandas dataframe for test set\n",
    "    df_Xtest = pd.DataFrame(X_test, columns=attributes)\n",
    "    df_ytest = pd.DataFrame(y_test, columns=['target'])\n",
    "    df_test = pd.concat([df_Xtest, df_ytest], axis = 1)\n",
    "\n",
    "    num_classes = df_ytrain.nunique()[0]\n",
    "\n",
    "    return df_train, df_test, num_classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgb(0, 255, 255, 0.2);\"><h5>def load_unlabeledDataframe()</h5></div>\n",
    "\n",
    "Provides the unlabeled version of dataset.\n",
    "\n",
    "Takes: \n",
    "- dataset: name of dataset (e.g. CRICKET_X)\n",
    "\n",
    "Returns:\n",
    "- df_past: pandas dataframe of past segments\n",
    "- df_pos: pandas dataframe of future segments in positive relation with past segments\n",
    "- df_neg: pansas dataframe of future segments in negative relation with past segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_unlabeledDataframe(dataset = None):\n",
    "\n",
    "    print(f'Dataset: {dataset}')\n",
    "\n",
    "    #load dataset\n",
    "    train = np.loadtxt(f'../Datasets/{dataset}/{dataset}_TRAIN.tsv').astype(np.float32)\n",
    "\n",
    "    #features only\n",
    "    X_train = train[:, 1:] # each row, columns from 1 to n\n",
    "\n",
    "    #X_train = np.array([np.array([1,2,3,4,5,6,7,8,9,10]), np.array([11,12,13,14,15,16,17,18,19,20])])\n",
    "\n",
    "    len_train = len(X_train[1])\n",
    "    len_past = int(len(X_train[1])*alpha)\n",
    "    len_future = len_train - len_past\n",
    "    roll_param = int(np.random.randint(len_future)*0.75) #roll \n",
    "\n",
    "    #np.roll shift all segement on right of roll_param positions\n",
    "\n",
    "    #print(f'len(train): {len_train}, len_past: {len_past}, len_future: {len_future}, roll_param: {roll_param}')\n",
    "\n",
    "    past = X_train[:, 0:len_past] #past segments (past) form 0 to len_past-1\n",
    "    future_pos = X_train[:, len_past:len_train] #future positive segments from len_past to len_train\n",
    "    future_neg = np.roll(X_train[:, len_past:len_train], roll_param) #future negative segments form len_past to len_train, and roll\n",
    "\n",
    "\n",
    "    #create a list of name for attributes past, future_positive and future_negative segments\n",
    "    att_past = [f'past{i}' for i in range(len_past)]\n",
    "    att_pos = [f'pos{i}' for i in range(len_future)]\n",
    "    att_neg = [f'neg{i}' for i in range(len_future)]\n",
    "\n",
    "    #create a pandas dataframe for past, future_positive and future_negative segments\n",
    "    df_past = pd.DataFrame(past, columns=att_past)\n",
    "    df_pos = pd.DataFrame(future_pos, columns=att_pos)\n",
    "    df_neg = pd.DataFrame(future_neg, columns=att_neg)\n",
    "\n",
    "    return df_past, df_pos, df_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: CricketX\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test, num_classes = load_labeledDataframe(dataset=CRICKET_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: CricketX\n"
     ]
    }
   ],
   "source": [
    "df_past, df_pos, df_neg = load_unlabeledDataframe(dataset=CRICKET_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def magnitude_warp_s(x, sigma=0.2, knot=4, plot=False):\n",
    "\n",
    "#     print(f'x.shape: {x.shape}')\n",
    "#     print(f'x: {x}')\n",
    "#     print(f'np.ones((x.shape[1], 1)).shape: {np.ones((x.shape[1], 1)).shape}')\n",
    "#     print(f'np.ones((x.shape[1], 1)): {np.ones((x.shape[1], 1))}')\n",
    "#     print(f'np.linspace(0, x.shape[0] - 1., num=knot + 2)).shape: {np.linspace(0, x.shape[0] - 1., num=knot + 2).shape}')\n",
    "#     print(f'np.linspace(0, x.shape[0] - 1., num=knot + 2)): {np.linspace(0, x.shape[0] - 1., num=knot + 2)}')\n",
    "\n",
    "#     from scipy.interpolate import CubicSpline\n",
    "#     orig_steps = np.arange(x.shape[0])\n",
    "\n",
    "#     random_warps = np.random.normal(loc=1.0, scale=sigma, size=(1, knot + 2, x.shape[1]))\n",
    "#     warp_steps = (np.ones((x.shape[1], 1)) * (np.linspace(0, x.shape[0] - 1., num=knot + 2))).T\n",
    "\n",
    "#     print(f'warp_steps: {warp_steps}')\n",
    "\n",
    "#     li = []\n",
    "#     for dim in range(x.shape[1]):\n",
    "#         li.append(CubicSpline(warp_steps[:, dim], random_warps[0, :, dim])(orig_steps))\n",
    "#     warper = np.array(li).T\n",
    "\n",
    "#     x_ = x * warper\n",
    "\n",
    "#     #if plot:\n",
    "#     #    hlp.plot1d(x, x_, save_file='aug_examples/magnitude_warp_s.png')\n",
    "    \n",
    "#     return x_\n",
    "\n",
    "\n",
    "# def time_warp_s(x, sigma=0.2, knot=4, plot=False):\n",
    "#     from scipy.interpolate import CubicSpline\n",
    "#     orig_steps = np.arange(x.shape[0])\n",
    "\n",
    "#     random_warps = np.random.normal(loc=1.0, scale=sigma, size=(1, knot + 2, x.shape[1]))\n",
    "#     warp_steps = (np.ones((x.shape[1], 1)) * (np.linspace(0, x.shape[0] - 1., num=knot + 2))).T\n",
    "\n",
    "#     ret = np.zeros_like(x)\n",
    "#     for dim in range(x.shape[1]):\n",
    "#         time_warp = CubicSpline(warp_steps[:, dim],\n",
    "#                                 warp_steps[:, dim] * random_warps[0, :, dim])(orig_steps)\n",
    "#         scale = (x.shape[0] - 1) / time_warp[-1]\n",
    "#         ret[:, dim] = np.interp(orig_steps, np.clip(scale * time_warp, 0, x.shape[0] - 1),\n",
    "#                                    x[:, dim]).T\n",
    "#     #if plot:\n",
    "#     #    hlp.plot1d(x, ret, save_file='aug_examples/time_warp_s.png')\n",
    "    \n",
    "#     return ret\n",
    "\n",
    "\n",
    "# class MagnitudeWrap:\n",
    "#     def __init__(self, sigma, knot, p):\n",
    "#         self.sigma = sigma\n",
    "#         self.knot = knot\n",
    "#         self.p = p\n",
    "\n",
    "#     def __call__(self, data):\n",
    "#         print('### MagnitudeWrap')\n",
    "\n",
    "#         if random.random() < self.p:\n",
    "#             return self.forward(data)\n",
    "\n",
    "#         return data\n",
    "\n",
    "#     def forward(self, data):\n",
    "#         return magnitude_warp_s(data, sigma=self.sigma, knot=self.knot)\n",
    "\n",
    "\n",
    "# class TimeWarp:\n",
    "#     def __init__(self, sigma, knot, p):\n",
    "#         self.sigma = sigma\n",
    "#         self.knot = knot\n",
    "#         self.p = p\n",
    "\n",
    "#     def __call__(self, data):\n",
    "#         print('### TimeeWrap')\n",
    "\n",
    "#         if random.random() < self.p:\n",
    "#             return self.forward(data)\n",
    "\n",
    "#         return data\n",
    "\n",
    "#     def forward(self, data):\n",
    "#         return time_warp_s(data, sigma=self.sigma, knot=self.knot)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgb(0, 255, 255, 0.2);\"><h5>class LabeledDataset()</h5></div>\n",
    "\n",
    "Class for labeled dataset. \n",
    "\n",
    "Takes: \n",
    "- dataframe: pandas dataframe of dataset\n",
    "\n",
    "Methods:\n",
    "- len: returns the length of dataframe\n",
    "- getitem: returns the pair (feature, tarhet) of sample at index 'idx' of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, dataframe=None):\n",
    "\n",
    "        self.df = dataframe\n",
    "\n",
    "        #self.magnitude_warp = MagnitudeWrap(sigma=0.3, knot=4, p=0.3)\n",
    "        #self.time_warp = TimeWarp(sigma=0.2, knot=8, p=0.3)\n",
    "        #self.transform = transform = transforms.Compose([self.magnitude_warp, self.time_warp])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.target = torch.from_numpy(np.array(self.df['target'][idx])).to(torch.int32)\n",
    "        self.features = torch.from_numpy(np.array(self.df.iloc[idx:idx+1, :len(self.df.columns)-1])).to(torch.float32)\n",
    "\n",
    "        #self.features = self.transform(self.features)\n",
    "\n",
    "        return  self.features, self.target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgb(0, 255, 255, 0.2);\"><h5>class UnlabeledDataset()</h5></div>\n",
    "\n",
    "Class for unlabeled dataset. \n",
    "\n",
    "Takes: \n",
    "- df_past: pandas dataframe of past segments\n",
    "- df_pos: pandas dataframe of future segments in positive relation with past segments\n",
    "- df_neg: pansas dataframe of future segments in negative relation with past segments\n",
    "\n",
    "Methods:\n",
    "- len: returns the length of dataframe (df_past, df_pos and df_neg have same length) \n",
    "- getitem: returns three samples, each at index 'idx' of df_past, df_pos and df_neg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, df_past=None, df_pos=None, df_neg=None):\n",
    "\n",
    "        self.df_past = df_past\n",
    "        self.df_pos = df_pos\n",
    "        self.df_neg = df_neg\n",
    "\n",
    "        #self.magnitude_warp = MagnitudeWrap(sigma=0.3, knot=4, p=0.3)\n",
    "        #self.time_warp = TimeWarp(sigma=0.2, knot=8, p=0.3)\n",
    "        #self.transform = transform = transforms.Compose([self.magnitude_warp, self.time_warp])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_past) # len is the same for all df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.features_past = torch.from_numpy(np.array(self.df_past.iloc[idx:idx+1,:])).to(torch.float32)\n",
    "        self.features_pos = torch.from_numpy(np.array(self.df_pos.iloc[idx:idx+1,:])).to(torch.float32)\n",
    "        self.features_neg = torch.from_numpy(np.array(self.df_neg.iloc[idx:idx+1,:])).to(torch.float32)\n",
    "\n",
    "        #self.features = self.transform(self.features)\n",
    "\n",
    "        return  self.features_past, self.features_pos, self.features_neg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgb(0, 255, 255, 0.2);\"><h5>class BackboneEncoder()</h5></div>\n",
    "\n",
    "Implementation of Backbone Encoder model. This model is used for features extraction of samples.\n",
    "\n",
    "Takes: \n",
    "- num_features: number of features in output by BackboneEncoder\n",
    "\n",
    "Input:\n",
    "- tensor of shape [batch_size, 1, num_samples]\n",
    "\n",
    "Output:\n",
    "- tensor of shape [batch_size, num_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackboneEncoder(nn.Module):\n",
    "    def __init__(self, num_features=None):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(num_features=8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=8, out_channels=16, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(num_features=16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(num_features=32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(num_features=self.num_features),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(output_size=1)\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.view(x.shape[0], 1, -1) # shape = (batch_size, 1, num_col)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        #print(f'layer1: {x.shape}')\n",
    "        x = self.layer2(x)\n",
    "        #print(f'layer2: {x.shape}')\n",
    "        x = self.layer3(x)\n",
    "        #print(f'layer3: {x.shape}')\n",
    "        x = self.layer4(x)\n",
    "        #print(f'layer4: {x.shape}')\n",
    "        x = self.flatten(x)\n",
    "        #print(f'flatten: {x.shape}')\n",
    "        x = F.normalize(x, dim=1)\n",
    "        #print(f'normalize: {x.shape}')\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgb(0, 255, 255, 0.2);\"><h5>class ClassificationHead()</h5></div>\n",
    "\n",
    "Implementation of Classification Head model. This model is used for multiclass classification samples in labeled dataset. Is lies after the Backbone Encoder, which has [batch_size, num_features] as output shape.\n",
    "\n",
    "Takes: \n",
    "- num_features: number of features in output by BackboneEncoder\n",
    "\n",
    "Input:\n",
    "- tensor of shape [batch_size, num_features]\n",
    "\n",
    "Output:\n",
    "- tensor of shape [batch_size, num_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, num_features=None, num_classes=None):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        self.fc = nn.Linear(self.num_features, self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgb(0, 255, 255, 0.2);\"><h5>class RelationHead()</h5></div>\n",
    "\n",
    "Implementation of Relation Head model. This model is used for binary classification of relation between past-future segments. Is lies after the Backbone Encoder, which has [batch_size, num_features] as output shape.\n",
    "\n",
    "Takes: \n",
    "- num_features: number of features in output by BackboneEncoder\n",
    "\n",
    "Input:\n",
    "- tensor of shape [batch_size, num_features]\n",
    "\n",
    "Output:\n",
    "- tensor of shape [batch_size, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationHead(nn.Module):\n",
    "    def __init__(self, num_features=None):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.fc1 = nn.Linear(2*self.num_features, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.leakyReLU = nn.LeakyReLU()\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.leakyReLU(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 300])\n"
     ]
    }
   ],
   "source": [
    "labeledDataset = LabeledDataset(dataframe=df_train)\n",
    "labeledTrain = DataLoader(dataset=labeledDataset, batch_size=128, shuffle=True)\n",
    "itl = iter(labeledTrain)\n",
    "xl = next(itl)[0]\n",
    "print(xl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 120])\n",
      "torch.Size([128, 1, 180])\n",
      "torch.Size([128, 1, 180])\n"
     ]
    }
   ],
   "source": [
    "unlabeledDataset = UnlabeledDataset(df_past=df_past, df_pos=df_pos, df_neg=df_neg)\n",
    "unlabeledTrain = DataLoader(dataset=unlabeledDataset, batch_size=128, shuffle=True)\n",
    "itu = iter(unlabeledTrain)\n",
    "xu = next(itu)\n",
    "\n",
    "print(xu[0].shape)\n",
    "print(xu[1].shape)\n",
    "print(xu[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [],
   "source": [
    "backboneEncoder = BackboneEncoder(num_features=num_features)\n",
    "classificationHead = ClassificationHead(num_features=num_features, num_classes=num_classes)\n",
    "relationHead = RelationHead(num_features=num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 12])\n"
     ]
    }
   ],
   "source": [
    "ol1 = backboneEncoder(xl)\n",
    "ol2 = classificationHead(ol1)\n",
    "print(ol2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([128, 128])\n",
      "torch.Size([128, 128])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([256, 1])\n"
     ]
    }
   ],
   "source": [
    "ou1 = backboneEncoder(xu[0])\n",
    "ou2 = backboneEncoder(xu[1])\n",
    "ou3 = backboneEncoder(xu[2])\n",
    "\n",
    "print(ou1.shape)\n",
    "print(ou2.shape)\n",
    "print(ou3.shape)\n",
    "\n",
    "xu1 = torch.cat((ou1, ou2), dim=1)\n",
    "xu2 = torch.cat((ou1, ou3), dim=1)\n",
    "\n",
    "print(xu1.shape)\n",
    "print(xu2.shape)\n",
    "\n",
    "ou4 = relationHead(xu1)\n",
    "ou5 = relationHead(xu2)\n",
    "\n",
    "print(ou4.shape)\n",
    "print(ou5.shape)\n",
    "\n",
    "xu3 = torch.cat((ou4, ou5), dim=0)\n",
    "\n",
    "print(xu3.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgb(0, 255, 255, 0.2);\"><h5>def accuracy()</h5></div>\n",
    "\n",
    "Implements accuracy metrics, relying on torchmetrics.Accuracy and torchmetrics.classification.BinaryAccuracy methods.\n",
    "\n",
    "Takes: \n",
    "- predictions: predictions makes by model\n",
    "- targets: labels of samples in dataset\n",
    "- num_classes: number of classes of dataset\n",
    "\n",
    "Returns:\n",
    "- accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions=None, targets=None, num_classes=None):\n",
    "    if num_classes == 2:\n",
    "        accuracy = BinaryAccuracy()\n",
    "    else:    \n",
    "        accuracy = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        \n",
    "    return accuracy(predictions, targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgb(0, 255, 255, 0.2);\"><h5>class EarlyStopping()</h5></div>\n",
    "\n",
    "Implementation of early stopping technique.\n",
    "\n",
    "Takes: \n",
    "- patience: number of epochs with abs(prev_loss-loss) <= threshold without stopping, where 'prev_loss' is the loss recordered in the previous epoch and 'loss' is the loss computed in the current epoch\n",
    "- threshold: below this number there isn't learning between previous epoch and current epoch\n",
    "\n",
    "Returns:\n",
    "- True: if the counter has reached the number 'patience'\n",
    "- False: otherwise\n",
    "\n",
    "Method:\n",
    "- init: initializes counter=0 and prev_loss=0\n",
    "- check: checks if 'counter' has reached the number 'patience' or update it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, patience=None, threshold=None):\n",
    "        super().__init__()\n",
    "        self.patience=patience #number of epochs with abs(prev_loss-loss) <= threshold without stopping\n",
    "        self.threshold=threshold \n",
    "        self.counter=0 #counter of epochs with abs(prev_loss-loss) <= threshold\n",
    "        self.prev_loss=0 #loss of previous epoch\n",
    "    \n",
    "    def __check__(self, loss=None):\n",
    "\n",
    "        if self.counter==0: #if counter == 0 -> first epoch\n",
    "            self.prev_loss=loss\n",
    "            return False\n",
    "\n",
    "        elif self.counter==patience: #if counter == patience -> stop \n",
    "            return True\n",
    "\n",
    "        else:   # else update counter or initialize it to 0\n",
    "            if abs(self.prev_loss-loss) <= self.threshold:\n",
    "                self.counter+=1\n",
    "            else:\n",
    "                self.counter=0\n",
    "            return False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgb(0, 255, 255, 0.2);\"><h5>def model_crt_opt()</h5></div>\n",
    "\n",
    "Provides models, criterions, and optimizer.\n",
    "\n",
    "Takes: \n",
    "- device: cpu/gpu\n",
    "\n",
    "Returns:\n",
    "- models: backbone encoder, classification head, relation head\n",
    "- criterions: cross entropy loss (for supervised learning), binary cross entropy loss (for self-supervised learning)\n",
    "- optimizer: Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_crt_opt(device=None):\n",
    "\n",
    "    backboneEncoder = BackboneEncoder(num_features=num_features).to(device)\n",
    "    classificationHead = ClassificationHead(num_features=num_features, num_classes=num_classes).to(device)\n",
    "    relationHead = RelationHead(num_features=num_features).to(device)\n",
    "\n",
    "    ce = nn.CrossEntropyLoss().to(device)\n",
    "    bce = nn.BCELoss().to(device)\n",
    "    opt = torch.optim.Adam([{'backboneParams':backboneEncoder.parameters(), \n",
    "                            'clfHeadParams':classificationHead.parameters(),\n",
    "                            'rltHeadParams':relationHead.parameters()}], lr=learning_rate)\n",
    "\n",
    "    return backboneEncoder, classificationHead, relationHead, ce, bce, opt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgb(0, 255, 255, 0.2);\"><h5>def supervised_learning_epoch()</h5></div>\n",
    "Implements supervised learning algorithm for one epoch.\n",
    "\n",
    "Takes: \n",
    "- training: flag to choose between training and evaluation phase\n",
    "- num_classes: number of classes of dataset (used for compute accuracy)\n",
    "- loader: dataloader (from torch). \n",
    "- backboneEncoder: BackboneEncoder model for features extracting \n",
    "- classificationHead: classificationHead model for classification\n",
    "- crossEntropy: cross entropy loss (from torch)\n",
    "- optimizer: any kind of optimizer (eg. Adam from torch)\n",
    "- device: cpu/gpu\n",
    "\n",
    "Returns:\n",
    "- loss: loss of this epoch\n",
    "- acc: accuracy of this epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervised_learning_epoch(training=None, num_classes=None, loader=None, backboneEncoder=None, \n",
    "                            classificationHead=None, crossEntropy=None, optimizer=None, device=None):\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "\n",
    "    if training:\n",
    "        backboneEncoder.train()\n",
    "        classificationHead.train()\n",
    "\n",
    "    else:\n",
    "        backboneEncoder.eval()\n",
    "        classificationHead.eval()\n",
    "\n",
    "    for i, data_lab in enumerate(loader):\n",
    "        features, targets = data_lab\n",
    "        features, targets = features.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = backboneEncoder(features)\n",
    "        outputs = classificationHead(outputs)\n",
    "\n",
    "        loss = crossEntropy(outputs, targets)\n",
    "\n",
    "        if training:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += accuracy(predictions=outputs, targets=targets, num_classes=num_classes)\n",
    "\n",
    "    loss = total_loss/len(loader)\n",
    "    acc = total_acc/len(loader)\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgb(0, 255, 255, 0.2);\"><h5>def selfsupervised_learning_epoch()</h5></div>\n",
    "Implements self-supervised learning algorithm for one epoch.\n",
    "\n",
    "Takes: \n",
    "- training: flag to choose between training and evaluation phase\n",
    "- loader: dataloader (from torch). \n",
    "- backboneEncoder: BackboneEncoder model for features extracting \n",
    "- relationHead: relationHead model for binary classification of past-future segments\n",
    "- binaryCrossEntropy: binary cross entropy loss (from torch)\n",
    "- optimizer: any kind of optimizer (eg. Adam from torch)\n",
    "- device: cpu/gpu\n",
    "\n",
    "Returns:\n",
    "- loss: loss of this epoch\n",
    "- acc: accuracy of this epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selfsupervised_learning_epoch(training=None, loader=None, backboneEncoder=None, relationHead=None, \n",
    "                                binaryCrossEntropy=None, optimizer=None, device=None):\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "\n",
    "    if training:\n",
    "        backboneEncoder.train()\n",
    "        relationHead.train()\n",
    "\n",
    "    else:\n",
    "        backboneEncoder.eval()\n",
    "        relationHead.eval()\n",
    "\n",
    "\n",
    "    for i, data in enumerate(loader):\n",
    "        past_features, pos_features, neg_features = data\n",
    "        past_features, pos_features, neg_features = past_features.to(device), pos_features.to(device), neg_features.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output_past = backboneEncoder(past_features)\n",
    "        output_pos = backboneEncoder(pos_features)\n",
    "        output_neg = backboneEncoder(neg_features)\n",
    "        \n",
    "        pos_segments = torch.cat((output_past, output_pos), dim=1)\n",
    "        neg_segments = torch.cat((output_past, output_neg), dim=1)\n",
    "\n",
    "        output_pos = relationHead(pos_segments)\n",
    "        output_neg = relationHead(neg_segments)\n",
    "\n",
    "        target_pos = torch.ones(output_pos.shape).to(torch.int32)\n",
    "        target_neg = torch.zeros(output_neg.shape).to(torch.int32)\n",
    "\n",
    "        outputs = torch.cat((output_pos, output_neg), dim=0)\n",
    "        targets = torch.cat((target_pos, target_neg), dim=0)\n",
    "\n",
    "        loss = binaryCrossEntropy(outputs, targets)\n",
    "\n",
    "        if training:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += accuracy(predictions=outputs, targets=targets, num_classes=2)\n",
    "\n",
    "    loss = total_loss/len(loader)\n",
    "    acc = total_acc/len(loader)\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgb(0, 255, 255, 0.2);\"><h5>def training()</h5></div>\n",
    "Implements training algorithm. Computes the supervised training and self-supervised training for all epochs.\n",
    "\n",
    "Takes: \n",
    "- labeledDataset: dataset with labels for supervised traning\n",
    "- num_classes: number of classes of labeledDataset\n",
    "- unlabeledDataset: dataset without labels for self-supervised traning\n",
    "- backboneEncoder: BackboneEncoder model for features extracting \n",
    "- classificationHead: classificationHead model for classification\n",
    "- relationHead: relationHead model for binary classification of past-future segments\n",
    "- crossEntropy: cross entropy loss (from torch)\n",
    "- binaryCrossEntropy: binary cross entropy loss (from torch)\n",
    "- optimizer: any kind of optimizer (eg. Adam from torch)\n",
    "- device: cpu/gpu\n",
    "\n",
    "Returns:\n",
    "- train_scores: tuple of 4 numpy arrays:\n",
    "  - total_train_loss_sl: numpy array that contains all loss scores recorderd during supervised training\n",
    "  - total_train_loss_ssl: numpy array that contains all loss scores recorderd during self-supervised training\n",
    "  - total_train_acc_sl: numpy array that contains all accuracy scores recorderd during supervised training\n",
    "  - total_train_acc_ssl: numpy array that contains all accuracy scores recorderd during self-supervised training\n",
    "- eval_scores: tuple of 4 numpy arrays:\n",
    "  - total_val_loss_sl: numpy array that contains all loss scores recorderd during supervised evaluation\n",
    "  - total_val_loss_ssl: numpy array that contains all loss scores recorderd during self-supervised evaluation\n",
    "  - total_val_acc_sl: numpy array that contains all accuracy scores recorderd during supervised evaluation\n",
    "  - total_val_acc_ssl: numpy array that contains all accuracy scores recorderd during self-supervised evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(labeledDataset=None, num_classes=None, unlabeledDataset=None,\n",
    "            backboneEncoder=None, classificationHead=None, relationHead=None,\n",
    "            crossEntropy=None, binaryCrossEntropy=None, optimizer=None, device=None):\n",
    "\n",
    "    # sl: supervised learning\n",
    "    # ssl: self-supervised learning\n",
    "\n",
    "    len_train = int(np.floor((2/3)*len(labeledDataset)))\n",
    "    len_val = int(np.ceil((1/3)*len(labeledDataset)))\n",
    "    train_set_lab, validation_set_lab = random_split(labeledDataset, [len_train, len_val])\n",
    "    train_set_unlab, validation_set_unlab = random_split(unlabeledDataset, [len_train, len_val])\n",
    "    \n",
    "    train_loader_lab = DataLoader(dataset=train_set_lab, batch_size=batch_size, shuffle=True)\n",
    "    val_loader_lab = DataLoader(dataset=validation_set_lab, batch_size=batch_size, shuffle=True)\n",
    "    train_loader_unlab = DataLoader(dataset=train_set_unlab, batch_size=batch_size, shuffle=True)\n",
    "    val_loader_unlab = DataLoader(dataset=validation_set_unlab, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    earlyStopping_sl = EarlyStopping(patience=patience, threshold=threshold)\n",
    "    earlyStopping_ssl = EarlyStopping(patience=patience, threshold=threshold)\n",
    "\n",
    "    total_train_loss_sl = np.array([])\n",
    "    total_train_loss_ssl = np.array([])\n",
    "    total_train_acc_sl = np.array([])\n",
    "    total_train_acc_ssl = np.array([])\n",
    "\n",
    "    total_val_loss_sl = np.array([])\n",
    "    total_val_loss_ssl = np.array([])\n",
    "    total_val_acc_sl = np.array([])\n",
    "    total_val_acc_ssl = np.array([])\n",
    "\n",
    "    for k in range(num_epochs):\n",
    "\n",
    "        #TRAINING\n",
    "\n",
    "        train_loss_sl, train_acc_sl = supervised_learning_epoch(training=True, num_classes=num_classes, loader_lab=train_loader_lab, \n",
    "                                                                backboneEncoder=backboneEncoder, classificationHead=classificationHead, \n",
    "                                                                crossEntropy=crossEntropy, optimizer=optimizer, device=device)\n",
    "\n",
    "        train_loss_ssl, train_acc_ssl = selfsupervised_learning_epoch(training=True, loader_lab=train_loader_unlab, \n",
    "                                                                    backboneEncoder=backboneEncoder, relationHead=relationHead, \n",
    "                                                                    binaryCrossEntropy=binaryCrossEntropy, optimizer=optimizer, device=device)\n",
    "\n",
    "\n",
    "        #VALIDATION\n",
    "\n",
    "        val_loss_sl, val_acc_sl = supervised_learning_epoch(training=False, num_classes=num_classes, loader_lab=val_loader_lab, \n",
    "                                                            backboneEncoder=backboneEncoder, classificationHead=classificationHead, \n",
    "                                                            crossEntropy=crossEntropy, optimizer=optimizer, device=device)\n",
    "\n",
    "        val_loss_ssl, val_acc_ssl = selfsupervised_learning_epoch(training=False, loader_lab=val_loader_unlab, \n",
    "                                                                    backboneEncoder=backboneEncoder, relationHead=relationHead, \n",
    "                                                                    binaryCrossEntropy=binaryCrossEntropy, optimizer=optimizer, device=device)\n",
    "        \n",
    "        total_train_loss_sl.append(train_loss_sl)\n",
    "        total_train_loss_ssl.append(train_loss_ssl)\n",
    "        total_train_acc_sl.append(train_acc_sl)\n",
    "        total_train_acc_ssl.append(train_acc_ssl)\n",
    "\n",
    "        total_val_loss_sl.append(val_loss_sl)\n",
    "        total_val_loss_ssl.append(val_loss_ssl)\n",
    "        total_val_acc_sl.append(val_acc_sl)\n",
    "        total_val_acc_ssl.append(val_acc_ssl)\n",
    "        \n",
    "        if earlyStopping_sl.__check__(loss=val_loss_sl) and earlyStopping_ssl.__check__(loss=val_loss_ssl):\n",
    "            break\n",
    "\n",
    "    train_loss_sl = total_train_loss_sl.mean()\n",
    "    train_loss_ssl = total_train_loss_ssl.mean()\n",
    "    train_acc_sl = total_train_acc_sl.mean()\n",
    "    train_acc_ssl = total_train_acc_ssl.mean()\n",
    "\n",
    "    val_loss_sl = total_val_loss_sl.mean()\n",
    "    val_loss_ssl = total_val_loss_ssl.mean()\n",
    "    val_acc_sl = total_val_acc_sl.mean()\n",
    "    val_acc_ssl = total_val_acc_ssl.mean()\n",
    "\n",
    "    train_scores = (total_train_loss_sl, total_train_loss_ssl, total_train_acc_sl, total_train_acc_ssl)\n",
    "    eval_scores = (total_val_loss_sl, total_val_loss_ssl, total_val_acc_sl, total_val_acc_ssl)\n",
    "        \n",
    "    return train_scores, eval_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "training(labeledDataset=labeledDataset, unlabeledDataset=unlabeledDataset, backboneEncoder=backboneEncoder, classificationHead=classificationHead, relationHead=relationHead)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
